{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chunking 101\n",
    "\n",
    "> Paired with Github Copilot\n",
    "\n",
    "RAG技术的探讨大多从向量数据库中检索出文档开始，即已经**分好片**（chunked），还带有**元数据**（metadata）的文本，然后作为和LLM交流的提示词上文，再和Agent结合，如推理、使用工具、函数调用等等。\n",
    "\n",
    "我们似乎默认向量数据库中的文档是已经分好片的，但是实际上，文档的分片是一个很复杂的问题，需要根据具体的应用场景来决定。这个过程叫做Chunking，一般来说分为两步：\n",
    "- 第一步整理原始文档，去除各种噪音；\n",
    "- 第二步拆分原书文档，变成合理的片段。\n",
    "\n",
    "现实中我们面对的文档（尤其是长文档）可能是：\n",
    "- 各种格式的文档，例如：Word, PDF, HTML, Markdown等等\n",
    "- 长度不一的文档，有的文档可能只有几十字，有的文档可能有几十万字\n",
    "- 文档内容有不同的语言，有的文档是中文，有的文档是英文\n",
    "\n",
    "在拆分片段的时候我们要考虑：\n",
    "- 文档作者自己的内容组织逻辑，可能是按照章节划分，也可能是按照主题划分\n",
    "- 向量数据库和LLM的交互，需要考虑LLM的输入长度限制，以及LLM的理解能力\n",
    "- 最后文档片段的使用场景，如提供答案、推理、使用工具、函数调用等等\n",
    "- 元信息的提取，如标题、作者、时间、来源等等\n",
    "\n",
    "上述这些都是需要根据具体的应用场景来决定的，因此需要频繁的调整和优化，也就意味着在实现RAG的时候需要不断的迭代和优化Chunking的过程。\n",
    "\n",
    "在这个Notebook中，我们将由浅入深讨论如何对文档进行Chunking，以及如何使用LangChain进行Chunking。\n",
    "\n",
    "> 我们可能要根据实际应用的场景不断调整分片，因此我们也期望尽量使用类似LangChain这样的库提供的开箱即用的函数，不需要太多自己的定制。\n"
   ],
   "id": "9d5fc9a4804f7021"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install -q rich langchain\n",
    "\n",
    "def print_docs_in_online(docs):\n",
    "    for doc in docs:\n",
    "        print(str(len(doc.page_content)) + \": \" + doc.page_content[:35]+ \"...\"+doc.page_content[-35:])\n",
    "            \n",
    "# from rich import print # 让输出更好看一点:)"
   ],
   "id": "ff897ef789ede712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "以一篇线上博客为为例：https://www.qinyu.info/post/wardley-maps/ch1/ （使用Hugo搭建）\n",
    "\n",
    "Web文档通常是html格式，内容有结构组织，都是通过标签表示。Web文档内容有这样一些特点：\n",
    "- 标记语言，Html格式本身就是文本，文本处理的效率较高\n",
    "- 文字内容有分段落，段落一般是p标签\n",
    "- 除了文本内容外还有标题，标题一般是h1, h2, h3等标签（作为meta信息，供后续加工）\n",
    "- 在团队/组织/公司内，Web文档都是批量存放（Wiki/Confluence...），html 结构相对固定，可以批量处理（例如上面博客网站中所有博客的Html结构都是一致的）\n",
    "- 不同的Web文档，html结构可能不同，需要根据**实际情况**（？）来解析\n",
    "\n",
    "Web结构化内容的解析在自动化测试和爬虫中间已经有了有很多成熟的应用，Python也有不少封装好的库可以使用，比如BeautifulSoup4, selenium，playwright等等。\n",
    "LangChain也有提供基本的[WebBaseLoader](https://python.langchain.com/docs/integrations/document_loaders/web_base/)，安装好LangChain就可以直接使用。\n",
    "\n",
    "> 除了WebBaseLoader，LangChain还提供了功能接近的[*URLLoader](https://python.langchain.com/docs/integrations/document_loaders/url/)，包括了：PlaywrightURLLoader和SeleniumURLLoader。区别是WebBaseLoader直接使用http client请求Web页面，而Playwright和Selenium则需要通过浏览器（headless）操作。从效率上来说WebBaseLoader更高，但是如果是动态内容（例如JavaScript渲染的页面或是依赖操作）就需要使用Playwright或Selenium。\n",
    "\n",
    "## 简单加载Web文档\n",
    "\n",
    "下面是一个简单的例子，展示如何使用WebBaseLoader加载Web文档。"
   ],
   "id": "778c796ba4e77b0"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "base_loader = WebBaseLoader(\"https://www.qinyu.info/post/wardley-maps/ch1/\")\n",
    "docs = base_loader.load()\n",
    "# print(docs)\n",
    "# print_docs_len(docs)\n",
    "print_docs_in_online(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1bd62870201cbb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 自定义WebLoader处理特定的Web页面\n",
    "\n",
    "我们可以看到，WebBaseLoader提供的基本功能把Web页面上的所有文字全部无差别地转换成了一个Document，这会带来很多不便。\n",
    "- 长度太长\n",
    "- 丢失了作者对内容的组织逻辑（章节）\n",
    "- 很多无价值干扰信息，如banner，侧边栏等等\n",
    "\n",
    "理解文章内容并结合Inspect后发现：\n",
    "- 文章的主题内容全部在页面右侧class为post__content的div标签下，banner和侧边栏的内容不是我们需要的\n",
    "- 文章的标题是是第一个h1标签，这是文章加载出来的所有Document都需要的的meta信息（最基本的摘要）\n",
    "- 每个段落是p标签，这些标签的文字内容是我们需要的\n",
    "- 段落中间还有h1标签，其后一组段落组成一个章节，h1标签是章节的小标题，这是章节需要的meta信息\n",
    "\n",
    "因此对于这个特定博客的解析不能简单地使用WebBaseLoader，需要自定义一个Loader，来解析这个特定的博客页面。"
   ],
   "id": "87ae9b89fff5823e"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import Iterator\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SectionWebBaseLoader(WebBaseLoader):\n",
    "    \n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "        for path in self.web_paths:\n",
    "            soup: BeautifulSoup = self._scrape(path, bs_kwargs=self.bs_kwargs) \n",
    "            # 通过观察页面结构，找到文章内容的div\n",
    "            post = soup.find(name=\"div\", attrs={\"class\":\"post__content\"})\n",
    "            \n",
    "            title = None\n",
    "            section_content = \"\"\n",
    "            for child in post.children:\n",
    "                text = child.get_text()\n",
    "                if child.name == \"h1\":\n",
    "                    # 如果是h1标签，那么这个是一个新的章节\n",
    "                    section = text\n",
    "                    # 如果是第一个h1标签，那么这个是文章的标题\n",
    "                    if title is None:\n",
    "                        title = text\n",
    "                        continue\n",
    "                    \n",
    "                    if section_content is not None:\n",
    "                        # 如果不是第一个章节，把上一个章节的内容提取成Document\n",
    "                        yield Document(page_content=section_content.strip(),\n",
    "                                       metadata={\"source\": path, \n",
    "                                                 \"title\": title,\n",
    "                                                 \"section\": section})\n",
    "                    section_content = \"\"\n",
    "                elif child.name == \"p\":\n",
    "                    # 如果是p标签，那么这个是章节内容的一段\n",
    "                    section_content = \"\".join([section_content, text])\n",
    "\n",
    "base_loader = SectionWebBaseLoader(\"https://www.qinyu.info/post/wardley-maps/ch1/\")\n",
    "docs = base_loader.load()\n",
    "# print(docs)\n",
    "# print_docs_len(docs)\n",
    "print_docs_in_online(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3626bb70bb553c5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 对整理后的文档进行分片\n",
    "\n",
    "我们可以看到，通过自定义的Loader，我们已经把文章内容按照章节（即作者的内容组织逻辑）进行了整理，但是这个时候的文档一部分还是太长，而且文档长度不一，需要进一步对文档继续分片。\n",
    "我们期望进一步分片后每个片段的长度差不多是均匀的，且片段的完整性没有破坏（即至少是一句话）。\n",
    "\n",
    "\n",
    "接下来我们就来看看LangChain提供的文本分片器，如何对文档进行分片。\n",
    "- 基于自然语言处理的分片器（NLTK、SPACY）\n",
    "- 基于句子的分片器（sentence-transformers）\n",
    "- 基于字符的分片器\n"
   ],
   "id": "c67a75eee66e0a90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install -q nltk spacy sentence-transformers\n",
    "# !python -m spacy download en_core_web_sm # 下载spacy的模型"
   ],
   "id": "5b68c470bc20bb47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_docs(documents, splitter):\n",
    "    _docs = []\n",
    "    for doc in documents:\n",
    "        chunked_text = splitter.split_text(doc.page_content)\n",
    "        for text in chunked_text:\n",
    "            _docs.append(Document(page_content=text,\n",
    "                                  metadata=doc.metadata))\n",
    "    return _docs\n",
    "\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# \n",
    "# import spacy\n",
    "# spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from langchain.text_splitter import NLTKTextSplitter, SpacyTextSplitter, SentenceTransformersTokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "nltk_splitter = NLTKTextSplitter()\n",
    "spacy_splitter = SpacyTextSplitter(max_length=3000)\n",
    "sentence_transformers_splitter = SentenceTransformersTokenTextSplitter()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                               chunk_overlap=50,\n",
    "                                               # keep_separator=True,\n",
    "                                               is_separator_regex=True,\n",
    "                                               separators=[\"\\n\", \"\\r\\n\", \"\\r\", \"。\", \"？\", \"！\", \"；\", \";\", \"!\", \"?\"])\n",
    "\n",
    "chunked_docs = split_docs(docs, text_splitter)\n",
    "# print(chunked_docs)\n",
    "print_docs_in_online(chunked_docs)"
   ],
   "id": "34ad28aab91349e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 第三方中文分片器\n",
    "\n",
    "上面我们看到用LangChain提供的开箱即用的分片器：\n",
    "- 处理中文文本非常地拉胯\n",
    "- 按照句子拆分不必NLP这样的牛刀\n",
    "\n",
    "> 但如果想从分片中提取实体或者关键字作为metadata，仍然需要利用NLP，但需要选择合适中文的NLP库，如jieba、THULAC、HanLP等等。（TODO：下一步调研）\n",
    "\n",
    "我们可以使用第三方的中文分片器，例如：https://github.com/chatchat-space/Langchain-Chatchat/blob/master/text_splitter/chinese_recursive_text_splitter.py"
   ],
   "id": "2499a2f7ab85d15a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from typing import List, Optional, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _split_text_with_regex_from_end(\n",
    "        text: str, separator: str, keep_separator: bool\n",
    ") -> List[str]:\n",
    "    # Now that we have the separator, split the text\n",
    "    if separator:\n",
    "        if keep_separator:\n",
    "            # The parentheses in the pattern keep the delimiters in the result.\n",
    "            _splits = re.split(f\"({separator})\", text)\n",
    "            splits = [\"\".join(i) for i in zip(_splits[0::2], _splits[1::2])]\n",
    "            if len(_splits) % 2 == 1:\n",
    "                splits += _splits[-1:]\n",
    "            # splits = [_splits[0]] + splits\n",
    "        else:\n",
    "            splits = re.split(separator, text)\n",
    "    else:\n",
    "        splits = list(text)\n",
    "    return [s for s in splits if s != \"\"]\n",
    "\n",
    "\n",
    "class ChineseRecursiveTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(\n",
    "            self,\n",
    "            separators: Optional[List[str]] = None,\n",
    "            keep_separator: bool = True,\n",
    "            is_separator_regex: bool = True,\n",
    "            **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "        self._separators = separators or [\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \"。|！|？\",\n",
    "            \"\\.\\s|\\!\\s|\\?\\s\",\n",
    "            \"；|;\\s\",\n",
    "            \"，|,\\s\"\n",
    "        ]\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        final_chunks = []\n",
    "        # Get appropriate separator to use\n",
    "        separator = separators[-1]\n",
    "        new_separators = []\n",
    "        for i, _s in enumerate(separators):\n",
    "            _separator = _s if self._is_separator_regex else re.escape(_s)\n",
    "            if _s == \"\":\n",
    "                separator = _s\n",
    "                break\n",
    "            if re.search(_separator, text):\n",
    "                separator = _s\n",
    "                new_separators = separators[i + 1:]\n",
    "                break\n",
    "\n",
    "        _separator = separator if self._is_separator_regex else re.escape(separator)\n",
    "        splits = _split_text_with_regex_from_end(text, _separator, self._keep_separator)\n",
    "\n",
    "        # Now go merging things, recursively splitting longer texts.\n",
    "        _good_splits = []\n",
    "        _separator = \"\" if self._keep_separator else separator\n",
    "        for s in splits:\n",
    "            if self._length_function(s) < self._chunk_size:\n",
    "                _good_splits.append(s)\n",
    "            else:\n",
    "                if _good_splits:\n",
    "                    merged_text = self._merge_splits(_good_splits, _separator)\n",
    "                    final_chunks.extend(merged_text)\n",
    "                    _good_splits = []\n",
    "                if not new_separators:\n",
    "                    final_chunks.append(s)\n",
    "                else:\n",
    "                    other_info = self._split_text(s, new_separators)\n",
    "                    final_chunks.extend(other_info)\n",
    "        if _good_splits:\n",
    "            merged_text = self._merge_splits(_good_splits, _separator)\n",
    "            final_chunks.extend(merged_text)\n",
    "        return [re.sub(r\"\\n{2,}\", \"\\n\", chunk.strip()) for chunk in final_chunks if chunk.strip()!=\"\"]\n"
   ],
   "id": "8f88fe82210ebf2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chunked_docs = split_docs(docs, ChineseRecursiveTextSplitter(chunk_size=500, chunk_overlap=50))\n",
    "# print(chunked_docs)\n",
    "print_docs_in_online(chunked_docs)"
   ],
   "id": "32ebf88ac1e611cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "最终，我们得到了一篇Web文档的分片：\n",
    "- 没有任何与主题无关的干扰信息\n",
    "- 分片遵循文档原有的章节组织\n",
    "- 分片的长度差不多均匀\n",
    "- 分片没有破坏句子的完整性\n",
    "- 分片除了source还有其他有价值的元信息（章节）\n",
    "\n",
    "整个处理过程没有用到NLP或者LLM，仅是文本处理。处理过程的效率还不错。"
   ],
   "id": "a2af0fb7b29410b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 总结\n",
    "\n",
    "- 标记语言格式的文档的干扰信息容易排除（排除/保留特定标签）\n",
    "- 标记语言格式的文档容易提取元信息\n",
    "- 文档格式（标记）和文档内容组织（章节）有关系，一般需要分析并自定义处理\n",
    "- 文档的分片一般使用基于字符的分片器就够了。（TODO：Embedding和检索时间效果待验证）\n",
    "- 对LangChain提供的开箱即用的DocumentLoader和TextSplitter的能力有一个初步的了解\n",
    "\n",
    "下一步工作（Chunking 102）：\n",
    "- 了解更多的文档格式，如Word, PDF, Markdown等等\n",
    "- 表格以及非结构化内容的处理，如图片等等\n",
    "- 文本分片必要的语义处理（提取实体或者关键字、摘要等等）\n",
    "\n",
    "文本分片以及元信息在具体RAG场景中使用的效果\n",
    "\n",
    "## 参考\n",
    "https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/8a30b5710b3dd99ef2239fb60c7b54bc38d3613d/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n"
   ],
   "id": "b96bc1daa2066463"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
